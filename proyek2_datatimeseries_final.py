# -*- coding: utf-8 -*-
"""Proyek2_DataTimeSeries (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-DdUDHybJS4Ok6RT3eWLWdsSiRsL6KQR

# Mendownload Dataset dan Import library

## Menginstall Package
"""

!pip install -q kaggle

import numpy as np
import pandas as pd
from keras.layers import Dense, LSTM
import matplotlib.pyplot as plt
import tensorflow as tf
from google.colab import files
import tensorflow as tf
import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Bidirectional, Dropout
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.preprocessing import MinMaxScaler

files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!ls ~/.kaggle

!kaggle datasets download -d mateuszk013/warsaw-daily-weather

"""## Pembuatan direktori dan unzip file"""

!warsaw-daily-weather
!unzip warsaw-daily-weather.zip -d warsaw-daily-weather
!ls warsaw-daily-weather

df = pd.read_csv('warsaw-daily-weather/warsaw.csv')

"""# Proses Data Cleaning"""

#Menampilkan isi dari dataset
df

df.info()

"""Disini saya hanya menggunakan TAVG yang adalah Average Temperature menjadi model pembelajaran machine learning saya."""

df_baru = df[["DATE", "TAVG"]]
df_baru = df_baru.copy()
df_baru['DATE'] = pd.to_datetime(df_baru['DATE'])

df_baru.info()

df_baru.isnull().sum()

"""# Data Processing"""

df_baru = df_baru.set_index('DATE')
scaler = MinMaxScaler(feature_range = (0, 1))
data_baru1 = scaler.fit_transform(df_baru)

tavg_column = data_baru1[:, df_baru.columns.get_loc("TAVG")]
threshold_mae = (tavg_column.max() - tavg_column.min()) * 10 / 100
print(f"Nilai Mae adalah {threshold_mae}%")

plt.plot(data_baru1)

"""## Pemisahan data train dan data test"""

train_size = int(len(data_baru1)*0.8)
test_size = len(data_baru1) - train_size
X_data, y_data = data_baru1[0: train_size, :], data_baru1[train_size: len(data_baru1), :1]
train_size, test_size

def dataset(dataset, time_step=1):
    X, Y = [], []
    for i in range(len(dataset)-time_step-1):
        a = dataset[i:(i+time_step), 0]
        X.append(a)
        Y.append(dataset[i + time_step, 0])
    return np.array(X), np.array(Y)

X_train, y_train = dataset(X_data, 100)
X_test, y_test = dataset(y_data, 100)
X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)

"""## Pembuatan Model"""

model = Sequential([
    Bidirectional(LSTM(64, return_sequences=True), input_shape=(100, 1)),
    Dropout(0.1),
    Bidirectional(LSTM(64)),
    Dropout(0.1),
    Dense(30, activation="relu"),
    Dense(10, activation="relu"),
    Dense(1),
])

"""# Testing Data"""

early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

optimizer = tf.keras.optimizers.SGD(learning_rate=1.0000e-04, momentum=0.9)
model.compile(optimizer='adam', loss=tf.keras.losses.Huber(), metrics=['mae'])

history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=128, verbose=1, callbacks=early_stopping)

loss = history.history['loss']
val_loss = history.history['val_loss']
mae = history.history['mae']
val_mae = history.history['val_mae']

plt.plot(loss)
plt.plot(val_loss)
plt.title('Model Loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

plt.plot(mae)
plt.plot(val_mae)
plt.title('Model mae')
plt.ylabel('mae')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()